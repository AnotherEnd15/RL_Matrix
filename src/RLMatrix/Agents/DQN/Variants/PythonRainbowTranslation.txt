# DuelingDQN_C51_Noisy.py

import torch
from torch import nn

class DuelingDQN_C51_Noisy(nn.Module):
    def __init__(self, name, obs_size, width, action_sizes, depth, num_atoms):
        super().__init__()
        self.name = name

        if obs_size < 1:
            raise ValueError("Number of observations can't be less than 1")

        self._num_atoms = num_atoms
        self._action_sizes = action_sizes

        # Shared layers configuration
        self._shared_modules = nn.ModuleList()
        self._shared_modules.append(nn.Linear(obs_size, width))  # First shared layer
        for _ in range(1, depth):
            self._shared_modules.append(nn.Linear(width, width))  # Additional shared layers

        # Separate value stream
        self._value_head = nn.Linear(width, self._num_atoms)  # C51 modifies this to output a distribution

        # Separate advantage streams for different action sizes
        self._advantage_heads = nn.ModuleList()
        for action_size in action_sizes:
            self._advantage_heads.append(nn.Linear(width, action_size * self._num_atoms))  # C51 modifications for distributional outputs

    def forward(self, x):
        if x.dim() == 1:
            x = x.unsqueeze(0)  # Ensure input is at least 2D, now [batch_size, feature_size]

        # Forward through shared layers
        for module in self._shared_modules:
            x = nn.functional.relu(module(x))  # After this, shape should still be [batch_size, feature_size]

        # Forward through value stream
        value = self._value_head(x)  # Shape [batch_size, 1, _num_atoms] for C51
        value = value.view(-1, 1, self._num_atoms)

        # Forward through each advantage stream (head)
        advantage_list = []
        for head in self._advantage_heads:
            advantage = head(x)  # Expected shape [batch_size, action_size * _num_atoms]
            num_actions = self._action_sizes[0]
            advantage = advantage.view(-1, num_actions, self._num_atoms)  # Change to [batch_size, num_actions, _num_atoms]
            advantage_list.append(advantage)

        # Combine value and advantages using dueling architecture
        q_distributions_list = []
        for advantage in advantage_list:
            q_distributions = value + (advantage - advantage.mean(dim=1, keepdim=True))  # Dueling architecture
            q_distributions = nn.functional.softmax(q_distributions, dim=-1)  # Apply softmax to normalize the distributions
            q_distributions_list.append(q_distributions)  # Now [batch_size, num_actions, _num_atoms]

        # Stack along a new dimension for the heads
        final_output = torch.stack(q_distributions_list, dim=1)  # [batch_size, num_heads, num_actions, _num_atoms]
        return final_output


# DQNAgentRainbow3.py

class DQNAgentRainbow3:
    def __init__(self, opts, envs, net_provider=None):
        self._v_min = opts.VMin
        self._v_max = opts.VMax
        self._num_atoms = opts.NumAtoms
        self._delta_z = (self._v_max - self._v_min) / (self._num_atoms - 1)

        self.target_net = RainbowNetworkProvider(opts.Width, opts.Depth, opts.NumAtoms).create_critic_net(envs[0])
        self.policy_net = RainbowNetworkProvider(opts.Width, opts.Depth, opts.NumAtoms).create_critic_net(envs[0])

    def select_action(self, state, is_training=True):
        if is_training:
            for module in self.policy_net.modules():
                if isinstance(module, nn.Linear):
                    module.reset_noise()

        with torch.no_grad():
            state_tensor = self.state_to_tensor(state)  # Shape: [state_dim]
            selected_actions = [0] * self.environments[0].action_size[0]

            q_values_all_heads = self.policy_net(state_tensor).view(1, len(self.environments[0].action_size), self.environments[0].action_size[0], self._num_atoms)  # Shape: [1, num_heads, num_actions, num_atoms]

            support = torch.linspace(self._v_min, self._v_max, steps=self._num_atoms).to(self.device)  # Shape: [num_atoms]

            for i in range(len(self.environments[0].action_size)):
                q_value_distribution = q_values_all_heads[0, i]  # Shape: [num_actions, num_atoms]
                expected_q_values = (q_value_distribution * support).sum(dim=-1)  # Shape: [num_actions]
                selected_actions[i] = int(expected_q_values.argmax().item())

            return selected_actions

    def optimize_model(self):
        if len(self.replay_buffer) < self.options.BatchSize:
            return

        transitions = self.replay_buffer.sample()
        sampled_indices = None

        if isinstance(self.replay_buffer, PrioritizedReplayMemory):
            sampled_indices = self.replay_buffer.get_sampled_indices()

        batch_states = [t.state for t in transitions]
        batch_multi_actions = [t.discrete_actions for t in transitions]
        batch_rewards = [t.reward for t in transitions]
        batch_next_states = [t.next_state for t in transitions]

        non_final_mask = self.create_non_final_mask(batch_next_states)
        next_state_values = self.compute_next_state_values(batch_next_states, non_final_mask)
        state_batch = self.create_state_batch(batch_states)
        action_batch = self.create_action_batch(batch_multi_actions)
        reward_batch = self.create_reward_batch(batch_rewards)

        q_values_all_heads = self.compute_q_values(state_batch)
        state_action_values = self.extract_state_action_values(q_values_all_heads, action_batch)

        expected_state_action_values = self.compute_expected_state_action_values(next_state_values, reward_batch, non_final_mask)

        loss = self.compute_loss(state_action_values, expected_state_action_values)

        self.update_model(loss)

        if sampled_indices is not None:
            self.update_prioritized_replay_memory(state_action_values, expected_state_action_values.detach(), sampled_indices)

    def compute_q_values(self, state_batch):
        return self.policy_net(state_batch)

    def extract_state_action_values(self, q_values_all_heads, action_batch):
        # Ensure action_batch is expanded to index properly into q_values_all_heads
        expanded_action_batch = action_batch.unsqueeze(-1).unsqueeze(-1).expand(-1, -1, -1, self._num_atoms)
        # Gather the Q-value distributions for the selected actions across all heads
        selected_action_distributions = q_values_all_heads.gather(2, expanded_action_batch).squeeze(2)
        return selected_action_distributions  # Shape: [batch_size, num_heads, num_atoms]

    def compute_next_state_values(self, batch_next_states, non_final_mask):
        non_final_next_states_array = [self.state_to_tensor(s) for s in batch_next_states if s is not None]

        if len(non_final_next_states_array) > 0:
            non_final_next_states = torch.stack(non_final_next_states_array).to(self.device)

            with torch.no_grad():
                # Use the target network to compute the distribution of Q-values for each next state
                next_q_distributions = self.target_net(non_final_next_states)  # Shape: [batch_size, num_heads, num_actions, num_atoms]

                # Compute the expected Q-value for each action by taking the sum of the distribution multiplied by the support
                expected_q_values = (next_q_distributions * torch.linspace(self._v_min, self._v_max, steps=self._num_atoms).to(self.device)).sum(dim=-1)  # Shape: [batch_size, num_heads, num_actions]

                # Select the best action for each head based on the expected Q-values
                best_actions = expected_q_values.argmax(dim=-1)  # Shape: [batch_size, num_heads]

                # Gather the corresponding Q-value distributions for the best actions
                best_q_distributions = next_q_distributions.gather(2, best_actions.unsqueeze(-1).unsqueeze(-1).expand(-1, -1, -1, self._num_atoms)).squeeze(2)  # Shape: [batch_size, num_heads, num_atoms]

                return best_q_distributions
        else:
            # All steps are final, likely this means all episodes are 1-step long.
            return torch.zeros(self.options.BatchSize, len(self.environments[0].action_size), self._num_atoms).to(self.device)

    def compute_expected_state_action_values(self, next_state_values, reward_batch, non_final_mask):
        # next_state_values: [batch_size_non_final, num_heads, num_atoms]
        # reward_batch: [batch_size]
        # non_final_mask: [batch_size]
        projected_dist = self.project_distribution(next_state_values)  # [batch_size_non_final, num_heads, num_atoms]
        masked_dist = torch.zeros(self.options.BatchSize, len(self.environments[0].action_size), self._num_atoms).to(self.device)  # [batch_size, num_heads, num_atoms]

        # Handle non-terminal states
        masked_dist.index_copy_(0, non_final_mask.nonzero().squeeze(), projected_dist)  # [batch_size, num_heads, num_atoms]

        # Handle terminal states
        terminal_mask = ~non_final_mask
        terminal_rewards = reward_batch[terminal_mask].unsqueeze(-1).unsqueeze(-1)
        terminal_dist = torch.zeros(terminal_mask.sum().item(), len(self.environments[0].action_size), self._num_atoms).to(self.device)

        atom_indices = ((terminal_rewards - self._v_min) / self._delta_z).round().to(torch.int64).clamp(0, self._num_atoms - 1).to(self.device)

        # Cast the source value to the same data type as terminal_dist
        scatter_source = torch.ones_like(terminal_dist).to(terminal_dist.dtype)

        terminal_dist.scatter_(2, atom_indices.expand(-1, len(self.environments[0].action_size), -1).to(self.device), scatter_source)

        masked_dist.index_copy_(0, terminal_mask.nonzero().squeeze(), terminal_dist)

        return masked_dist

    def project_distribution(self, distribution):
        # distribution: [batch_size_non_final, num_heads, num_atoms]
        projected = torch.zeros_like(distribution)  # [batch_size_non_final, num_heads, num_atoms]
        z_values = torch.arange(self._v_min, self._v_max + self._delta_z, self._delta_z).to(self.device)  # [num_atoms]

        batch_size_non_final = distribution.size(0)
        reward_batch_non_final = self.create_reward_batch(batch_size_non_final)  # [batch_size_non_final]

        for i in range(self._num_atoms):
            z_tilde = (reward_batch_non_final.unsqueeze(-1) + self.options.GAMMA * z_values[i]).clamp(self._v_min, self._v_max)  # [batch_size_non_final, num_heads]
            bj = (z_tilde - self._v_min) / self._delta_z  # [batch_size_non_final, num_heads]
            lj = bj.floor()  # [batch_size_non_final, num_heads]
            uj = bj.ceil()  # [batch_size_non_final, num_heads]

            lj_mask = (lj >= 0) & (lj < self._num_atoms)  # [batch_size_non_final, num_heads]
            uj_mask = (uj >= 0) & (uj < self._num_atoms)  # [batch_size_non_final, num_heads]

            lj_masked = lj.to(torch.int64) * lj_mask.to(torch.int64)  # [batch_size_non_final, num_heads]
            uj_masked = uj.to(torch.int64) * uj_mask.to(torch.int64)  # [batch_size_non_final, num_heads]

            lower_part = distribution.gather(2, lj_masked.unsqueeze(-1)).squeeze(-1) * (uj - bj) * lj_mask  # [batch_size_non_final, num_heads]
            upper_part = distribution.gather(2, uj_masked.unsqueeze(-1)).squeeze(-1) * (bj - lj) * uj_mask  # [batch_size_non_final, num_heads]

            projected.scatter_add_(2, lj_masked.unsqueeze(2), lower_part.unsqueeze(2))  # [batch_size_non_final, num_heads, num_atoms]
            projected.scatter_add_(2, uj_masked.unsqueeze(2), upper_part.unsqueeze(2))  # [batch_size_non_final, num_heads, num_atoms]

        return projected

    def compute_loss(self, state_action_distributions, target_distributions):
        criterion = nn.KLDivLoss(reduction='none')
        loss = criterion(state_action_distributions.log(), target_distributions.log()).mean(dim=2).sum()

        print(loss)

        return loss

    def update_model(self, loss):
        self.optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 100)
        self.optimizer.step()